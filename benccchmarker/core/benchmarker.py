import json

import pandas as pd
import numpy as np

from benccchmarker.metrics.classification import calculate_accuracy_score, calculate_auc_score, calculate_f1_score
from benccchmarker.metrics.similarity import calculate_jaccard_coefficient, calculate_dissimilarity, calculate_overlapped_elements

class Benchmarker():
    """
    Benchmarker class allows to compare the prediction file with the ground truth
    data from simulated datasets or curated datasets.

    Parameters
    ----------
    prediction_file_path : str
        Path to the prediction file by the methods you're running the dataset on,
        When performing this make sure that you also have the ground_truth to validate
        the prediction on, otherwise this will fail. Prediction file should contain
        'sender', 'receiver', 'ligand', 'receptor' columns.
    ground_truth_file_path : str
        Path to the ground truth data file, this file is generated by the data_generator
        module and contains 'sender', 'receiver', 'ligand', 'receptor' columns as well as 
        'label' column. Or if you want to use your own ground truth data, make sure it has
        the same columns as the generated ground truth data.
    ground_truth_data_params : str
        Path to the ground truth data parameters file. This file is generated by the data_generator module and contains all of the parameters used to generate the ground truth data.
    output_dir : str
        Path to the output directory where the results will be saved.
    """
    def __init__(self, prediction_file_path, ground_truth_file_path, ground_truth_data_params_path, output_dir):
        
        self.prediction_file_path = prediction_file_path
        self.prediction = pd.read_csv(prediction_file_path)

        self.ground_truth_file_path = ground_truth_file_path
        self.ground_truth = pd.read_csv(ground_truth_file_path)

        # Load ground_truth_data_params
        with open(ground_truth_data_params_path, 'r') as f:
            ground_truth_data_params = json.load(f)
        self.ground_truth_data_params = ground_truth_data_params

        self.output_dir = output_dir

        self.result = None

    def get_results(self):
        """
        Get the results of the benchmarking
        """
        return self.result
    
    def set_results(self, result):
        """
        Set the results of the benchmarking
        """
        self.result = result
        

    def run(self):
        """
        Compare the loaded prediction to the ground truth that is available
        """

        ground_truth_df = self.ground_truth
        prediction_df = self.prediction
        ground_truth_data_params = self.ground_truth_data_params

        required_columns = [
            "source",
            "target",
            "ligand",
            "receptor",
        ]
        input_columns = prediction_df.columns.str.lower()

        missing_columns = set(required_columns) - set(input_columns)
        print(missing_columns)

        if missing_columns:
            raise ValueError(f"Missing the following columns: {' '.join(missing_columns)}. Cannot proceed with the benchmarking, please make sure you have the required columns {required_columns}")
        else:

            predicted_labels = set(prediction_df[required_columns].apply(lambda row: '---'.join(row.values.astype(str)), axis=1))
            ground_truth_labels = ground_truth_df["label"].to_list()

            true_labels_count = ground_truth_data_params["num_cell_types"] ** 2 * ground_truth_data_params["num_lr_pairs"]

            true_positives = len(set(ground_truth_labels).intersection(set(predicted_labels)))
            false_positives = len(set(predicted_labels) - set(ground_truth_labels))
            false_negatives = len(set(ground_truth_labels) - set(predicted_labels))
            true_negatives = true_labels_count - true_positives - false_positives - false_negatives

            if true_negatives < 0:
                true_negatives = 0
            
            TPR = true_positives / (true_positives + false_negatives)
            FPR = false_positives / (false_positives + true_negatives)

            # Calculate AUC
            y_true = np.concatenate([np.ones(true_positives + false_negatives), np.zeros(false_positives + true_negatives)])
            y_scores = np.concatenate([np.ones(true_positives), np.zeros(false_positives), np.zeros(false_negatives), np.zeros(true_negatives)])

            auc = calculate_auc_score(y_true, y_scores)

            # Calculate jaccard index
            jaccard_index = true_positives / (true_positives + false_positives + false_negatives)

            result = {
                "file_path": self.prediction_file_path,
                "num_cells": ground_truth_data_params["num_cells"],
                "num_lr_pairs": ground_truth_data_params["num_lr_pairs"],
                "num_cell_types": ground_truth_data_params["num_cell_types"],
                "overexpression_scale": ground_truth_data_params["overexpression_scale"] if "overexpression_scale" in ground_truth_data_params else None,
                # Mean expression if exists otherwise None
                "mean_expression": ground_truth_data_params["mean_expression"] if "mean_expression" in ground_truth_data_params else None,
                "dispersion": ground_truth_data_params["dispersion"] if "dispersion" in ground_truth_data_params else None,
                "combination": true_labels_count, 
                "tp": true_positives,
                "tn": true_negatives,
                "fp": false_positives,
                "fn": false_negatives,
                "fpr": FPR,
                "tpr": TPR,
                "auc": auc,
                "ji": jaccard_index,
            }

            # Save result to a csv file
            result_df = pd.DataFrame([result])
            result_df.to_csv(f"{self.output_dir}/benchmarking_result.csv", index=False)

            self.set_results(result)
            print("Saved the benchmarking results to the output directory")


